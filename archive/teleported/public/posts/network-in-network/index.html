<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Network In Network architecture: The beginning of Inception // teleported.in</title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta property="og:title" content="Teleported.in" />
<meta property="og:description" content="A blog where I share my intuitions about artificial intelligence, machine learning, deep learning." />
<meta property="og:type" content="website" />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="http://teleported.in/posts/network-in-network/" />



    <link href="" rel="alternate" type="application/rss+xml" title="teleported.in" />
    <link rel="shortcut icon" type="image/png" href="/favicon.ico">

    <link href="http://teleported.in/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
    <link href="http://teleported.in/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="http://teleported.in/css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>


    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

    <meta name="description" content="A blog where I share my intuitions about artificial intelligence, machine learning, deep learning.">
    <meta name="keywords" content="ai ml artificial intelligence programming c++ python intuitions papers">
    <meta name="author" content="Anand Saha <anandsaha gmail com">

    <meta name="generator" content="Hugo 0.40.1" />
</head>


<body>
<div id="container">
    <header id="header">
    <div id="header-outer" class="outer">
        <div id="header-inner" class="inner">
            <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
            <a id="logo" class="logo-text" style="font-family: 'Open Sans', sans-serif;" href="http://teleported.in/">teleported.in</a>
            <nav id="main-nav">
                
                <a style="font-family: 'Open Sans', sans-serif;" class="main-nav-link" href="/about/about">About</a>
                
                <a style="font-family: 'Open Sans', sans-serif;" class="main-nav-link" href="/posts/">Posts</a>
                
            </nav>
            <nav id="sub-nav">
                <div id="search-form-wrap">
                </div>
            </nav>
        </div>
        <div style="margin-top:-10px; padding-bottom:5px">
        <label style="font-family: 'Open Sans', sans-serif;color:#555555">
            Deep learning concepts. Tools and techniques.</label>
        </div>
    </div>
</header>

    <section id="main" class="outer">
        <article class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        <header class="article-header">
            <h1 class="article-title" itemprop="name">Network In Network architecture: The beginning of Inception</h1>
        </header>
        
        <div class="article-meta">
            <a href="/posts/network-in-network/" class="article-date">
                <time datetime='2017-10-20T23:27:27.000-04:00' itemprop="datePublished">2017-10-20</time>
            </a>
            
            
            <div class="post-categories">
                <div class="article-category">
                    
                    
                    <a class="article-category-link" href="http://teleported.in//categories/deep-learning">deep learning</a>
                    
                </div>
            </div>
            
            
            
            <div class="article-comment-link-wrap">
                <a href="/posts/network-in-network/#disqus_thread" class="article-comment-link">Comments</a>
            </div>
            
        </div>
        <div style="color:#999999;">
        By <a style="text-decoration: none;color:#999999; border-bottom: 1px dotted #000;" href="/about/about">Anand Saha</a>
        </div>


        <div class="article-entry" itemprop="articleBody">
            

<h3 id="introduction">Introduction</h3>

<p>In this post, I explain the <a href="https://arxiv.org/abs/1312.4400"><code>Network In Network</code></a>  paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.</p>

<h3 id="motivation">Motivation</h3>

<p>Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:</p>

<p><center><img src="/post_imgs/11-LeNet.png" alt="LeNet" />
<em>Fig. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">LeNet-5</a></em>
</center></p>

<p>Yann LeCun&rsquo;s work (<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf">1989</a>, <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf">1998</a>) triggered the convolutional approach, which takes into account the inherent structure of the incoming data (mostly image data) while propagating them through the network and learning about them.</p>

<p>If you need a CNN refresher, <a href="http://cs231n.github.io/convolutional-networks/">this</a> is an excellent read.</p>

<p>The idea of convolution is very simple and a genius one. Images have spatial information (height, width, and channels) and this arrangement of pixels is important to understanding their contents. Conventional neural networks would flatten their input before applying the weights, thereby losing the spatial information.</p>

<p><center>
  <img src="/post_imgs/11-convnet.jpeg" alt="Typical arrangement in covnet" />
  <em>Fig. The typical arrangement of various layers in a CovNet (Img Credit: <a href="http://cs231n.github.io/convolutional-networks/">cs231n</a>)</em>
</center></p>

<p>Convolutional networks, on the other hand, operate directly on the images as is. The filters (also called kernels) are moved across the image left to right, top to bottom as if scanning the image and weighted sum of products are calculated between the filter and subset of the image the filter is superimposing on. This is the convolution operation. What is to be noted here is that the operation is <em>linear</em>. Of course these are then passed through various other operations like non-linear activations and pooling.</p>

<p><center>
<img src="/post_imgs/11-convolution.png" alt="convolution" />
<em>Fig: Conventional Convolution operation, and an idea (Img Credit: <a href="https://www.researchgate.net/figure/309487032%5ffig2%5fFigure-2-a-Illustration-of-the-operation-principle-of-the-convolution-kernel">ResearchGate</a> with my annotations)</em>
</center></p>

<p>The aspect to note in the convolution operation is the <em>linearity</em> of the operation. Does it need to be linear? Can it be something else that can extract richer features?</p>

<h3 id="the-idea-of-network-in-network">The idea of Network In Network</h3>

<p>This paper had a new take on how the convolution filters are designed and how we map extracted features to class scores. This formed the basis of the Inception architecture. Two new concepts were introduced in this CNN architecture design:</p>

<ul>
<li><strong>MLPconv</strong>: Replaced linear filters with nonlinear <code>Multi Linear Perceptrons</code> to extract better features within the receipt field (see the figure above). This helped in better abstraction and accuracy.</li>
<li><strong>Global Average Pooling</strong>: Got rid of the fully connected layers at the end thereby reducing parameters and complexity. This was replaced by the creation of as many activation maps in the last layer as there are classes. This was followed by averaging these maps to arrive at final scores, which is passed to softmax. This is performant and more intuitive.</li>
</ul>

<h3 id="mlpconv">MLPConv</h3>

<p>Traditional CNN architectures use linear filters to do the convolution and extract features out of images. The early layers try to extract primitive features like lines, edges, and corners, while the later layers build on early layers and extract higher-level features like eyes, ears, nose etc. These are called latent features.</p>

<p><center>
<img src="/post_imgs/11-zeiler-fertus.jpg" alt="Feature extraction" />
<em>Fig.Hierarchy of features extracted from various layers (Img Credit: <a href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</a>)</em>
</center></p>

<p>Now, there can be variations in each of those features - there can be many different variations in eyes alone for e.g.. A linear filter (for e.g. to detect eyes) tries to draw straight lines to extract these features. Thus conventional CNN implicitly makes the assumption that the latent concepts are linearly separable. But a straight line may not always fit. The separation of the various types of eye features and non-eye features may not be a straight line. Using a richer nonlinear function approximator can serve as a better feature extractor.</p>

<p><center><img src="/post_imgs/11-Network_In_Network_img1.png" alt="MLPConv" />
<em>Fig. Conventional linear convolution layer (Img Source: <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a> )</em>
</center></p>

<p>This paper introduced the concept of having a neural network itself in place of a convolution filter. The input to this mini network would be the convolution, and the output would be the value of a neuron in the activation. Hence it does not alter the input/output characteristics of traditional filters. This mini network, called MLPconv, can then convolved over the input. The benefit of having such an arrangement is two-fold:</p>

<ul>
<li>It is compatible with the backpropagation logic of neural nets, thus this fits well into existing architectures of CNN&rsquo;s</li>
<li>It can itself be a deep model leading to rich separation between latent features</li>
</ul>

<p><center><img src="/post_imgs/11-Network_In_Network_img2.png" alt="MLPConv" />
<em>Fig. MLPconv layer (Img Source: <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a> )</em>
</center></p>

<h3 id="global-average-pooling">Global Average Pooling</h3>

<p>In traditional CNN architectures, the feature maps of the last convolution layer are flattened and passed on to one or more fully connected layers, which are then passed on to softmax logistics layer for spitting out class probabilities. The issue with this approach is that it is hard to decode how the usual fully connected layers seen at the end of CNN architectures map to class probabilities. They are black boxes between the convolution layers and the classifier. They are also prone to overfitting and come with lots of parameters to train. An estimate says that the last FC layers contain 90% of the parameters of the network.</p>

<p>Can we do better?</p>

<p><center><img src="/post_imgs/11-Network_In_Network_img3.png" alt="MLPConv" />
<em>Fig. Global Average Pooling (Img Source: <a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a> )</em>
</center></p>

<p>In the approach proposed by the paper, the last  MLPconv layer produces as many activation maps as the number of classes being predicted. Then, each map is averaged giving rise to the raw scores of the classes. These are then fed to a SoftMax layer to produce the probabilities, totally making FC layers redundant.</p>

<p>The advantages of this approach are:</p>

<ul>
<li>The mapping between the extracted features and the class scores is more intuitive and direct. The feature can be treated as category confidence.</li>
<li>An implicit advantage is that there are no new parameters to train (unlike the FC layers), leading to less overfitting.</li>
<li>Global average pooling sums out the spatial information, thus it is more robust to spatial translations of the input.</li>
</ul>

<h3 id="conclusion">Conclusion</h3>

<p>The paper <code>demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets</code> but more importantly gave a new direction to the design of convolution filters.</p>

<p>References:</p>

<ul>
<li><a href="https://arxiv.org/abs/1312.4400">https://arxiv.org/abs/1312.4400</a></li>
<li><a href="https://openreview.net/forum?id=ylE6yojDR5yqX">https://openreview.net/forum?id=ylE6yojDR5yqX</a></li>
</ul>

<p><code>Please leave a comment below if anything was unclear or can be improved in the post.</code></p>

        </div>

        
        
        <div class="article-toc" style="display:none;">
            <h3>Contents</h3>
            <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#the-idea-of-network-in-network">The idea of Network In Network</a></li>
<li><a href="#mlpconv">MLPConv</a></li>
<li><a href="#global-average-pooling">Global Average Pooling</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
        </div>
        
        

        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.slim.min.js" integrity="sha256-/SIrNqv8h6QGKDuNoLGA4iret+kyesCkHGzVUUV0shc=" crossorigin="anonymous"></script>
        <script>
            (function() {
                var $toc = $('#TableOfContents');
                if ($toc.length > 0) {
                    var $window = $(window);

                    function onScroll(){
                        var currentScroll = $window.scrollTop();
                        var h = $('.article-entry h1, .article-entry h2, .article-entry h3, .article-entry h4, .article-entry h5, .article-entry h6');
                        var id = "";
                        h.each(function (i, e) {
                            e = $(e);
                            if (e.offset().top - 10 <= currentScroll) {
                                id = e.attr('id');
                            }
                        });
                        var active = $toc.find('a.active');
                        if (active.length == 1 && active.eq(0).attr('href') == '#' + id) return true;

                        active.each(function (i, e) {
                            $(e).removeClass('active').siblings('ul').hide();
                        });
                        $toc.find('a[href="#' + id + '"]').parentsUntil('#TableOfContents').each(function (i, e) {
                            $(e).children('a').addClass('active').siblings('ul').show();
                        });
                    }

                    $window.on('scroll', onScroll);
                    $(document).ready(function() {
                        $toc.find('a').parent('li').find('ul').hide();
                        onScroll();
                        document.getElementsByClassName('article-toc')[0].style.display = '';
                    });
                }
            })();
        </script>
        


        
        <footer class="article-footer">
            <ul class="article-tag-list">
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="http://teleported.in//tags/deep-learning">deep learning
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="http://teleported.in//tags/architecture">architecture
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="http://teleported.in//tags/inception">inception
                    </a>
                </li>
                
            </ul>
        </footer>
        
    </div>
    <nav id="article-nav">
    
    <a href="/posts/decoding-resnet-architecture/" id="article-nav-newer" class="article-nav-link-wrap">
        <div class="article-nav-title"><span>&lt;</span>&nbsp;
            Decoding the ResNet architecture
        </div>
    </a>
    
    
    <a href="/posts/intro-to-computer-vision/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Class Notes: Computer Vision (Georgia Tech)&nbsp;<span>&gt;</span></div>
    </a>
    
</nav>
</article>

        
            
        
    </section>
    <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            
            &copy; 2020 <a href="https://www.linkedin.com/in/anandsaha/">Anand Saha</a> &nbsp; 
            Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with theme <a href="https://github.com/carsonip/hugo-theme-minos">Minos</a>
        </div>
    </div>
    

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/tomorrow-night.min.css" integrity="sha256-2wL88NKUqvJi/ExflDzkzUumjUM73mcK2gBvBBeLvTk=" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js" integrity="sha256-KbfTjB0WZ8vvXngdpJGY3Yp3xKk+tttbqClO11anCIU=" crossorigin="anonymous"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    <script>
        document.getElementById('main-nav-toggle').addEventListener('click', function () {
            var header = document.getElementById('header');
            if (header.classList.contains('mobile-on')) {
                header.classList.remove('mobile-on');
            } else {
                header.classList.add('mobile-on');
            }
        });
    </script>
</footer>

</div>
</body>
</html>
