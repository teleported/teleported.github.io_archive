+++
date          = "2017-10-02T23:27:27-04:00"
title         = "An intuitive explaination of Batch Normalization"
description   = "An intuitive explaination of batch normalization"
tags          = [ "deep learning", "ai", "techniques"]
categories    = [ "deep learning" ]
slug          = "batch-normalization-explained"
featuredImage = "/post_imgs/10-bnalgorithm.png"
draft         = true
+++

The Batch Normalization technique was proposed by researchers Sergey Ioffe, Christian Szegedy in a [2015 paper] (https://arxiv.org/abs/1502.03167). Since then, it has become an integral part of any deep network architecture. Assignment 2 of the [cs231n course](http://cs231n.github.io/assignments2016/assignment2/) made us [implement](https://github.com/anandsaha/cs231n.assignments/blob/master/2016winter/assignment2/BatchNormalization.ipynb) the forward and backward pass of this strategy, which prompted me to write about it here.

A few concepts need to be pinned down before talking about batch normalization.

### Distribution of features

### Normalization

### Whitening



![training with batch normalization](/post_imgs/10-batchnormgraphs.png)

