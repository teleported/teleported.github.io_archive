<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Techniques on teleported.in</title>
    <link>http://teleported.in/tags/techniques/</link>
    <description>Recent content in Techniques on teleported.in</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Oct 2017 23:27:27 -0400</lastBuildDate>
    
	<atom:link href="http://teleported.in/tags/techniques/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>The Batch Normalization Technique</title>
      <link>http://teleported.in/posts/batch-normalization-explained/</link>
      <pubDate>Mon, 02 Oct 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/batch-normalization-explained/</guid>
      <description>The Batch Normalization technique was proposed by researchers Sergey Ioffe, Christian Szegedy in a 2015 paper called Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. Since then, it has become an integral part of any deep network architecture.
It has been one of the most exiting innovations in recent times, having impacted
The paper claims the following:
 It makes higher learning rates possible It makes proper weights initialization less critical Acts as a regularizer, in some cases eliminating the need for dropouts  We will be hashing each of this claims in this post.</description>
    </item>
    
  </channel>
</rss>