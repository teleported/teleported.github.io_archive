<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on teleported.in</title>
    <link>http://teleported.in/categories/deep-learning/</link>
    <description>Recent content in Deep Learning on teleported.in</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 May 2018 00:00:00 -0530</lastBuildDate>
    
	<atom:link href="http://teleported.in/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Deep Learning with PyTorch (video course)</title>
      <link>http://teleported.in/posts/20-dl-with-pytorch/</link>
      <pubDate>Tue, 01 May 2018 00:00:00 -0530</pubDate>
      
      <guid>http://teleported.in/posts/20-dl-with-pytorch/</guid>
      <description>I have been working on a video course with Packt Publications titled Deep Learning with PyTorch
Glad to announce that it got released today! (1st May 2018).
 
Course video description This video course will get you up-and-running with one of the most cutting-edge deep learning libraries: PyTorch. Written in Python, PyTorch is grabbing the attention of all data science professionals due to its ease of use over other libraries and its use of dynamic computation graphs.</description>
    </item>
    
    <item>
      <title>COCOB: An optimizer without a learning rate</title>
      <link>http://teleported.in/posts/cocob/</link>
      <pubDate>Thu, 28 Dec 2017 20:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/cocob/</guid>
      <description>At the nurture.ai&amp;rsquo;s NIPS paper implementation challenge, I implemented and validate the paper &amp;lsquo;Training Deep Networks without Learning Rates Through Coin Betting&amp;rsquo; using PyTorch. (github)
This paper caught my attention due to it&amp;rsquo;s promise to get rid of the learning rate hyper-parameter during model training.
The paper says: In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function.</description>
    </item>
    
    <item>
      <title>The Cyclical Learning Rate technique</title>
      <link>http://teleported.in/posts/cyclic-learning-rate/</link>
      <pubDate>Sun, 12 Nov 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/cyclic-learning-rate/</guid>
      <description>Introduction Learning rate (LR) is one of the most important hyperparameters to be tuned and holds key to faster and effective training of neural networks. Simply put, LR decides how much of the loss gradient is to be applied to our current weights to move them in the direction of lower loss.
 new_weight = existing_weight - learning_rate * gradient 
The step is simple. But as research has shown, there is so much that can be done to improve this step alone which has a profound influence on the training.</description>
    </item>
    
    <item>
      <title>Decoding the ResNet architecture</title>
      <link>http://teleported.in/posts/decoding-resnet-architecture/</link>
      <pubDate>Thu, 02 Nov 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/decoding-resnet-architecture/</guid>
      <description>Introduction Fast.ai&amp;rsquo;s 2017 batch kicked off on 30th Oct and Jeremy Howard introduced us participants to the ResNet model in the first lecture itself. I had used this model earlier in the passing but got curious to dig into its architecture this time. (In fact in one of my earlier client projects I had used Faster RCNN, which uses a ResNet variant under the hood.)
ResNet was unleashed in 2015 by Kaiming He.</description>
    </item>
    
    <item>
      <title>Network In Network architecture: The beginning of Inception</title>
      <link>http://teleported.in/posts/network-in-network/</link>
      <pubDate>Fri, 20 Oct 2017 23:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/network-in-network/</guid>
      <description>Introduction In this post, I explain the Network In Network paper by Min Lin, Qiang Chen, Shuicheng Yan (2013). This paper was quite influential in that it had a new take on convolutional filter design, which inspired the Inception line of deep architectures from Google.
Motivation Anyone getting introduced to convolutional networks first come across this familiar arrangement of neurons designed by Yann LeCun decades ago:
Fig. LeNet-5</description>
    </item>
    
    <item>
      <title>Detecting and counting objects in high-res aerial images</title>
      <link>http://teleported.in/posts/object-detection/</link>
      <pubDate>Sun, 01 Oct 2017 20:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/object-detection/</guid>
      <description>Recently I spent 4 months at Flytbase on an interesting problem: detection and counting of Oryxes, an endangered animal in the middle east, in very high resolution aerial images.
Using high resolution aerial images to train computer vision models poses unique challenges:
 Lack of sufficient training data: There are plenty of open training datasets out there, but almost all of them have images taken from human eye level. What makes aerial images unique is their top-down view of the objects.</description>
    </item>
    
  </channel>
</rss>