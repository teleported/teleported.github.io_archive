<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>The Cyclical Learning Rate technique // teleported.in</title>

    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta property="og:title" content="Teleported.in" />
<meta property="og:description" content="A blog where I share my intuitions about artificial intelligence, machine learning, deep learning." />
<meta property="og:type" content="website" />
<meta property="og:locale" content="en_US" />
<meta property="og:url" content="http://teleported.in/posts/cyclic-learning-rate/" />



    <link href="" rel="alternate" type="application/rss+xml" title="teleported.in" />
    <link rel="shortcut icon" type="image/png" href="/favicon.ico">

    <link href="http://teleported.in/webfonts/ptserif/main.css" rel='stylesheet' type='text/css'>
    <link href="http://teleported.in/webfonts/source-code-pro/main.css" rel="stylesheet" type="text/css">

    <link rel="stylesheet" href="http://teleported.in/css/style.css">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans" rel="stylesheet">

    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>


    <link href="http://gmpg.org/xfn/11" rel="profile">
    <meta http-equiv="content-type" content="text/html; charset=utf-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

    <meta name="description" content="A blog where I share my intuitions about artificial intelligence, machine learning, deep learning.">
    <meta name="keywords" content="ai ml artificial intelligence programming c++ python intuitions papers">
    <meta name="author" content="Anand Saha <anandsaha gmail com">

    <meta name="generator" content="Hugo 0.20.6" />
</head>


<body>
<div id="container">
    <header id="header">
    <div id="header-outer" class="outer">
        <div id="header-inner" class="inner">
            <a id="main-nav-toggle" class="nav-icon" href="javascript:;"></a>
            <a id="logo" class="logo-text" style="font-family: 'Open Sans', sans-serif;" href="http://teleported.in/">teleported.in</a>
            <nav id="main-nav">
                
                <a style="font-family: 'Open Sans', sans-serif;" class="main-nav-link" href="/about/about">About</a>
                
                <a style="font-family: 'Open Sans', sans-serif;" class="main-nav-link" href="/posts/">Posts</a>
                
            </nav>
            <nav id="sub-nav">
                <div id="search-form-wrap">
                </div>
            </nav>
        </div>
        <div style="margin-top:-10px; padding-bottom:5px">
        <label style="font-family: 'Open Sans', sans-serif;color:#555555">
            Deep learning concepts. Tools and techniques.</label>
        </div>
    </div>
</header>

    <section id="main" class="outer">
        <article class="article article-type-post" itemscope itemprop="blogPost">
    <div class="article-inner">
        
        <header class="article-header">
            <h1 class="article-title" itemprop="name">The Cyclical Learning Rate technique</h1>
        </header>
        
        <div class="article-meta">
            <a href="/posts/cyclic-learning-rate/" class="article-date">
                <time datetime='2017-11-12T23:27:27.000-04:00' itemprop="datePublished">2017-11-12</time>
            </a>
            
            
            <div class="post-categories">
                <div class="article-category">
                    
                    
                    <a class="article-category-link" href="http://teleported.in//categories/deep-learning">deep learning</a>
                    
                </div>
            </div>
            
            
            
            <div class="article-comment-link-wrap">
                <a href="/posts/cyclic-learning-rate/#disqus_thread" class="article-comment-link">Comments</a>
            </div>
            
        </div>
        <div style="color:#999999;">
        By <a style="text-decoration: none;color:#999999; border-bottom: 1px dotted #000;" href="/about/about">Anand Saha</a>
        </div>


        <div class="article-entry" itemprop="articleBody">
            

<h3 id="introduction">Introduction</h3>

<p>Learning rate (LR) is one of the most important hyperparameters to be tuned and holds key to faster and effective training of neural networks. Simply put, LR decides how much of the loss gradient is to be applied to our current weights to move them in the direction of lower loss.</p>

<p><center>
<code>new_weight = existing_weight - learning_rate * gradient</code>
</center></p>

<p>The step is simple. But as research has shown, there is so much that can be done to improve this step alone which has a profound influence on the training.</p>

<p>In this post, I explain <a href="https://arxiv.org/abs/1506.01186">Cyclical Learning Rate</a> (CLR), a very novel and simple idea to set and control the LR during training. It was covered by <a href="https://twitter.com/jeremyphoward">@jeremyphoward</a> in this year&rsquo;s <a href="http://www.fast.ai">fast.ai course</a>.</p>

<p>Note that CLR is very similar to <a href="https://arxiv.org/abs/1608.03983">Stochastic Gradient Descent with Warm Restarts</a> (SGDR), which says, &ldquo;<em>CLR is closely-related to our approach in its spirit and formulation but does not focus on restarts</em>.&rdquo; The <a href="https://github.com/fastai/fastai">fastai library</a> uses SGDR as the annealing schedule (with the idea of an LR finder from CLR).</p>

<h3 id="motivation">Motivation</h3>

<p>Neural networks are full of parameters that need to be trained to accomplish a certain task. <em>Training parameters</em> typically mean finding and setting appropriate values in them, so that they minimize a loss function with each batch of training.</p>

<p><center>
<img src='/post_imgs/15-neural_network-7.png' width=300px>
<em>Fig.: A simple neural network where the w&rsquo;s and b&rsquo;s are to be learnt (Img Credit: <a href="https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/">Matt Mazur</a>)</em>
</center></p>

<p>Traditionally, there has been broadly two approaches to setting the LR during training.</p>

<p><strong>One LR for all parameters</strong></p>

<p>Typically seen in <a href="https://en.wikipedia.org/wiki/Stochastic_gradient_descent">SGD</a>, a single LR is set at the beginning of the training, and an LR decay strategy is set (step, exponential etc.). This single LR is used to update all parameters. It is gradually decayed with each epoch with the assumption that with time, we reach near to the desired minima, upon which we need to slow down the updates so as not to overshoot it.</p>

<p><center>
<img src='/post_imgs/15-learningrates.jpeg' width=300px>
<em>Fig. Effect of various learning rates on convergence (Img Credit: <a href="http://cs231n.github.io/neural-networks-3/">cs231n</a>)</em>
</center></p>

<p>There are many challenges to this approach (<a href="https://arxiv.org/abs/1609.04747">refer</a>):</p>

<ul>
<li>Choosing an initial LR can be difficult to set in advance (<em>as depicted in above figure</em>).</li>
<li>Setting an LR schedule (LR update mechanism to decay it over time) is also difficult to be set in advance. They do not adapt to dynamics in data.</li>
<li>The same LR gets applied to all parameters which might be learning at different rates.</li>
<li>It is very hard to get out of a <a href="http://www.denizyuret.com/2015/03/alec-radfords-animations-for.html">saddle point</a>. See below.</li>
</ul>

<p><strong>Adaptive LR for each parameter</strong></p>

<p>Improved optimizers like <em>AdaGrad</em>, <em>AdaDelta</em>, <em>RMSprop</em> and <em>Adam</em> alleviate much of the above challenges by adapting learning rates for each parameters being trained. With Adadelta, we do not even need to set a default learning rate, as it has been eliminated from the update rule [2].</p>

<p><center>
<img src="/post_imgs/15-Beale.gif" alt="img" />
<em>Fig: Animation comparing optimization algorithms (Img Credit: <a href="https://www.reddit.com/r/MachineLearning/comments/2gopfa/visualizing%5fgradient%5foptimization%5ftechniques/cklhott/">Alec Radford</a>)</em>
</center></p>

<h3 id="cycling-learning-rate">Cycling Learning Rate</h3>

<p>CLR was proposed by Leslie Smith in 2015. It is an approach to LR adjustments where the value is cycled between a lower bound and upper bound. By nature, it is seen as a competitor to the adaptive LR approaches and hence used mostly with SGD. But it is possible to use it along with the improved optimizers (mentioned above) with per parameter updates.</p>

<p>CLR is computationally cheaper than the optimizers mentioned above. As the paper says:</p>

<p><code>Adaptive learning rates are fundamentally different from CLR policies, and CLR can be combined with adaptive
learning rates, as shown in Section 4.1. In addition, CLR policies are computationally simpler than adaptive learning rates. CLR is likely most similar to the SGDR method that appeared recently.</code>[1]</p>

<p><strong>Why it works</strong></p>

<p>As far as intuition goes, conventional wisdom says we have to keep decreasing the LR as training progresses so that we converge with time.</p>

<p>However, counterintuitively it might be useful to periodically vary the LR between a lower and higher threshold. The reasoning is that the periodic higher learning rates within the training help the model come out of any local minimas or saddle points if it ever enters into one. In fact, <em>Dauphin et al. [3] argue that the difficulty in minimizing the loss arises from saddle points rather than poor local minima.[1]</em> If the saddle point happens to be an elaborate plateau, lower learning rates can never generate enough gradient to come out of it (or will take enormous time). That&rsquo;s where periodic higher learning rates help with more rapid traversal of the surface.</p>

<p><center>
<img src='/post_imgs/15-saddlepoint.png' width=300px>
<em>Fig.: A saddle point in the error surface (Img Credit: <a href="https://www.safaribooksonline.com/library/view/fundamentals-of-deep/9781491925607/ch04.html">safaribooksonline</a>)</em>
</center></p>

<p>A second benefit is that the optimal LR appropriate for the error surface of your model will in all probability lie between the lower and higher bounds as discussed above. Hence we do get to use the best LR when amortized over time.</p>

<p><strong>Epoch, iterations, cycles and stepsize</strong></p>

<p>These terms have specific meaning in this algorithm, understanding them will make it easy to plug them in equations.</p>

<p>Let us consider a training dataset with 50,000 instances.</p>

<p>An <em>epoch</em> is one run of your training algorithm across the entire training set. If we set a batch size of 100, we get 500 batches in 1 epoch or 500 iterations. The iteration count is accumulated over epochs, so that in epoch 2, we get iterations 501 to 1000 for the same batch of 500, and so one.</p>

<p>With that in mind, a <em>cycle</em> is defined as that many <em>iterations</em> where we want our learning rate to go from a <em>base learning rate</em> to a <em>max learning rate</em>, and back. And a <em>stepsize</em> is half of a <em>cycle</em>. Note that a cycle, in this case, need not fall on the boundary of an epoch, though in practice it does.</p>

<p><center>
<img src="/post_imgs/15-clr-triangle.png" alt="clr step size" />
<em>Fig: Triangular LR policy. (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf">https://arxiv.org/pdf/1506.01186.pdf</a>)</em>
</center></p>

<p>In the above diagram, we set a <em>base lr</em> and <em>max lr</em> for the algorithm, demarcated by the red lines. The blue line suggests the way learning rate is modified (in a triangular fashion), with the x-axis being the iterations. A complete up and down of the blue line is one <em>cycle</em>. And <em>stepsize</em> is half of that.</p>

<p><strong>Calculating the LR</strong></p>

<p>As we gather from the above, the following needs to be fed into the algorithm for it to work:</p>

<ul>
<li>number of iterations that we want in a stepsize (half of a cycle)</li>
<li>base_lr</li>
<li>max_lr</li>
</ul>

<p>Later we will see that the optimal values of these can be programatically derived. Below is a piece of code which demonstrates the way LR is calculated:</p>

<pre><code>def get_triangular_lr(iteration, stepsize, base_lr, max_lr):
    &quot;&quot;&quot;Given the inputs, calculates the lr that should be applicable for this iteration&quot;&quot;&quot;
    cycle = np.floor(1 + iteration/(2  * stepsize))
    x = np.abs(iteration/stepsize - 2 * cycle + 1)
    lr = base_lr + (max_lr - base_lr) * np.maximum(0, (1-x))
    return lr

# Demo of how the LR varies with iterations
num_iterations = 10000
stepsize = 1000
base_lr = 0.0001
max_lr = 0.001
lr_trend = list()

for iteration in range(num_iterations):
    lr = get_triangular_lr(iteration, stepsize, base_lr, max_lr)
    # Update your optimizer to use this learning rate in this iteration
    lr_trend.append(lr)

plt.plot(lr_trend)
</code></pre>

<p><center>
<img src='/post_imgs/15-clr-graph.png' width=500px>
<em>Fig: Graph showing the variation of lr with iteration. We are using the triangular profile.</em>
</center></p>

<p>If you are a PyTorch user, note that there is a <a href="https://github.com/pytorch/pytorch/pull/2016">pull request</a> currently open in PyTorch queue to add this learning rate scheduler in PyTorch.</p>

<p><strong>Deriving the optimal <em>base lr</em> and <em>max lr</em></strong></p>

<p>An optimal lower and upper bound of the learning rate can be found by letting the model run for a few epochs, letting the learning rate increase linearly and monitoring the accuracy.</p>

<p>We run a complete step by setting stepsize equal to num_iterations (This will make the LR increase linearly and stop as num_iterations is reached). We also set <em>base lr</em> to a minimum value and <em>max lr</em> to a maximum value that we deem fit.</p>

<p>The accuracy plot will see an increase in accuracy as we increase the learning rate, but will plateau at a point and start decreasing again. Note the LR at which accuracy starts to increase, and also the LR when it starts stagnating. These are good points to set as <em>base lr</em> and <em>max lr</em></p>

<p><center>
<img src="/post_imgs/15-deciding-baselr-maxlr.png" alt="finding base lr" />
<em>Fig: Plot of accuracy vs learning rate (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf">https://arxiv.org/pdf/1506.01186.pdf</a>)</em>
</center></p>

<p>Alternatively, you can note the LR where accuracy peaks, and use that as <em>max lr</em>. Set <em>base lr</em> as <sup>1</sup>&frasl;<sub>3</sub> or <sup>1</sup>&frasl;<sub>4</sub> of this.</p>

<p><strong>Deriving the optimal cycle length (or stepsize)</strong></p>

<p>The paper suggests, after experimentation, that the stepsize be set to 2-10 times the number of iterations in an epoch. In the previous example, since we had 500 iterations per epoch, setting stepsize from 1000 to 5000 would do. The paper found not much difference in setting stepsize to 2 times num of iterations in an epoch than 8 times so.</p>

<h3 id="variants">Variants</h3>

<p>In addition to the triangular profile used above, the author also experimented with other functional forms.</p>

<p><strong>triangular2</strong>: Here the <em>max lr</em> is halved every cycle to bring down the difference between <em>base lr</em> and <em>max lr</em>.</p>

<p><center>
<img src="/post_imgs/15-triangular2.png" alt="triangular2" />
<em>Fig: Graph showing the variation of lr with iteration for the triangular2 approach (Img Credit: <a href="https://github.com/bckenstler/CLR">Brad Kenstler</a>)</em>
</center></p>

<p><strong>exp_range</strong>: Here the <em>max lr</em> is decayed exponentially with each iteration.</p>

<p><center>
<img src="/post_imgs/15-exp_range.png" alt="exp_range" />
<em>Fig: Graph showing the variation of lr with iteration for the exp-range approach (Img Credit: <a href="https://github.com/bckenstler/CLR">Brad Kenstler</a>)</em>
</center></p>

<p>The amplitude is adjusted either at the end of each mini batch, or at the end of a cycle[5]. These showed improvements in comparison with fixed learning rate and exponentially decaying learning rate respectively in the paper.</p>

<h3 id="results">Results</h3>

<p>CLR may provide quicker convergence on certain neural net tasks and architectures, hence it is something to try out.[5]</p>

<p><center>
<img src="/post_imgs/15-clr-cifar10.png" alt="CLR with Cifar10" />
<em>Fig. CLR tested on CIFAR 10 (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf">https://arxiv.org/pdf/1506.01186.pdf</a>)</em>
</center></p>

<p>In the above test, CLR took 25k iterations to reach an accuracy of 81%, which was reached in 70,000 iterations using traditional LR techniques.
<center>
<img src="/post_imgs/15-clr-adam.png" alt="CLR with Adam" />
<em>Fig. CLR used with Nesterov and Adam. Much faster convergence with Nesterov (Nesterov is an improvement over SGD) (Img Credit: <a href="https://arxiv.org/pdf/1506.01186.pdf">https://arxiv.org/pdf/1506.01186.pdf</a>)</em>
</center></p>

<p>In another test, CLR with Nesterov optimizer converged much quicker than Adam.</p>

<h3 id="conclusion">Conclusion</h3>

<p>CLR brings in a novel technique to manage the learning rate and can be used with SGD or with the advanced optimizers. CLR is one technique that should be in every deep learning practitioner&rsquo;s tool box.</p>

<p><strong>References</strong></p>

<ol>
<li><a href="https://arxiv.org/pdf/1506.01186.pdf">Cyclical Learning Rates for Training Neural Networks, Smith</a></li>
<li><a href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization algorithms, Rudder</a></li>
<li>Y. N. Dauphin, H. de Vries, J. Chung, and Y. Bengio. Rmsprop and equilibrated adaptive learning rates for non-convex optimization.</li>
<li><a href="https://arxiv.org/abs/1608.03983">SGDR: Stochastic Gradient Descent with Warm Restarts, Loshchilov, Hutter</a></li>
<li><a href="https://github.com/bckenstler/CLR">https://github.com/bckenstler/CLR</a></li>
</ol>

        </div>

        
        
        <div class="article-toc" style="display:none;">
            <h3>Contents</h3>
            <nav id="TableOfContents">
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#motivation">Motivation</a></li>
<li><a href="#cycling-learning-rate">Cycling Learning Rate</a></li>
<li><a href="#variants">Variants</a></li>
<li><a href="#results">Results</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
        </div>
        
        

        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.slim.min.js" integrity="sha256-/SIrNqv8h6QGKDuNoLGA4iret+kyesCkHGzVUUV0shc=" crossorigin="anonymous"></script>
        <script>
            (function() {
                var $toc = $('#TableOfContents');
                if ($toc.length > 0) {
                    var $window = $(window);

                    function onScroll(){
                        var currentScroll = $window.scrollTop();
                        var h = $('.article-entry h1, .article-entry h2, .article-entry h3, .article-entry h4, .article-entry h5, .article-entry h6');
                        var id = "";
                        h.each(function (i, e) {
                            e = $(e);
                            if (e.offset().top - 10 <= currentScroll) {
                                id = e.attr('id');
                            }
                        });
                        var active = $toc.find('a.active');
                        if (active.length == 1 && active.eq(0).attr('href') == '#' + id) return true;

                        active.each(function (i, e) {
                            $(e).removeClass('active').siblings('ul').hide();
                        });
                        $toc.find('a[href="#' + id + '"]').parentsUntil('#TableOfContents').each(function (i, e) {
                            $(e).children('a').addClass('active').siblings('ul').show();
                        });
                    }

                    $window.on('scroll', onScroll);
                    $(document).ready(function() {
                        $toc.find('a').parent('li').find('ul').hide();
                        onScroll();
                        document.getElementsByClassName('article-toc')[0].style.display = '';
                    });
                }
            })();
        </script>
        


        
        <footer class="article-footer">
            <ul class="article-tag-list">
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="http://teleported.in//tags/deep-learning">deep learning
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="http://teleported.in//tags/architecture">architecture
                    </a>
                </li>
                
                <li class="article-tag-list-item">
                    <a class="article-tag-list-link" href="http://teleported.in//tags/training">training
                    </a>
                </li>
                
            </ul>
        </footer>
        
    </div>
    <nav id="article-nav">
    
    <a href="/posts/attention/" id="article-nav-newer" class="article-nav-link-wrap">
        <div class="article-nav-title"><span>&lt;</span>&nbsp;
            COCOB: An optimizer with a learning rate
        </div>
    </a>
    
    
    <a href="/posts/decoding-resnet-architecture/" id="article-nav-older" class="article-nav-link-wrap">
        <div class="article-nav-title">Decoding the ResNet architecture&nbsp;<span>&gt;</span></div>
    </a>
    
</nav>
</article>

        
            <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'teleported-in';
    var disqus_identifier = 'http:\/\/teleported.in\/posts\/cyclic-learning-rate\/';
    var disqus_title = 'The Cyclical Learning Rate technique';
    var disqus_url = 'http:\/\/teleported.in\/posts\/cyclic-learning-rate\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
        
    </section>
    <footer id="footer">
    <div class="outer">
        <div id="footer-info" class="inner">
            
            &copy; 2018 <a href="https://www.linkedin.com/in/anandsaha/">Anand Saha</a> &nbsp; 
            Powered by <a href="https://gohugo.io" target="_blank">Hugo</a> with theme <a href="https://github.com/carsonip/hugo-theme-minos">Minos</a>
        </div>
    </div>
    
<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-98287631-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/tomorrow-night.min.css" integrity="sha256-2wL88NKUqvJi/ExflDzkzUumjUM73mcK2gBvBBeLvTk=" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js" integrity="sha256-KbfTjB0WZ8vvXngdpJGY3Yp3xKk+tttbqClO11anCIU=" crossorigin="anonymous"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    
    <script>
        document.getElementById('main-nav-toggle').addEventListener('click', function () {
            var header = document.getElementById('header');
            if (header.classList.contains('mobile-on')) {
                header.classList.remove('mobile-on');
            } else {
                header.classList.add('mobile-on');
            }
        });
    </script>
</footer>

</div>
</body>
</html>
