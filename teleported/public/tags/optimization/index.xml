<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Optimization on teleported.in</title>
    <link>http://teleported.in/tags/optimization/</link>
    <description>Recent content in Optimization on teleported.in</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Dec 2017 20:27:27 -0400</lastBuildDate>
    
	<atom:link href="http://teleported.in/tags/optimization/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>COCOB: An optimizer without a learning rate</title>
      <link>http://teleported.in/posts/cocob/</link>
      <pubDate>Thu, 28 Dec 2017 20:27:27 -0400</pubDate>
      
      <guid>http://teleported.in/posts/cocob/</guid>
      <description>At the nurture.ai&amp;rsquo;s NIPS paper implementation challenge, I implemented and validate the paper &amp;lsquo;Training Deep Networks without Learning Rates Through Coin Betting&amp;rsquo; using PyTorch. (github)
This paper caught my attention due to it&amp;rsquo;s promise to get rid of the learning rate hyper-parameter during model training.
The paper says: In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function.</description>
    </item>
    
  </channel>
</rss>